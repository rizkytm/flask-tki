{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/saeful/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-dc4d0455f222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'python youtube_api_cmd.py --c --videourl=https://www.youtube.com/watch?v=By_Cn5ixYLg --key=AIzaSyA7uGNpZO4WrnV6tWv2asX9wM145vc3t_U'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_command(command):\n",
    "    p = subprocess.Popen(command,\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.STDOUT)\n",
    "    return iter(p.stdout.readline, b'')\n",
    "command = 'python youtube_api_cmd.py --c --videourl=https://www.youtube.com/watch?v=By_Cn5ixYLg --key=AIzaSyA7uGNpZO4WrnV6tWv2asX9wM145vc3t_U'.split()\n",
    "for line in run_command(command):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Video_comment.txt','r') as f:\n",
    "    text_split = [line.rstrip('\\n') for line in f]\n",
    "review_text = pd.DataFrame()\n",
    "review_text['comment'] = text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Anyone thought of making a rewind of all the c...\n",
       "1          This is not realllll this is not realllllll\n",
       "2    in all honestly, this is lit. im actually amaz...\n",
       "3                                                Woooh\n",
       "4          *this video is unavailable in your country*\n",
       "5    40 million views and 7.3 million likes! Now th...\n",
       "6         Вообще 40 млн. Человек смотрят эту х***ю 😶🙄🤐\n",
       "7                                    WALMAAAAAAART!!!!\n",
       "8            I can't stop watching this. It's too good\n",
       "9                                                     \n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text['comment'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sample = review_text['comment'].values\n",
    "for i in range(len(doc_sample)):\n",
    "    doc_sample[i] = doc_sample[i].split()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['Anyone', 'thought', 'of', 'making', 'a', 'rewind', 'of', 'all', 'the', 'comments?'])\n",
      " list(['This', 'is', 'not', 'realllll', 'this', 'is', 'not', 'realllllll'])\n",
      " list(['in', 'all', 'honestly,', 'this', 'is', 'lit.', 'im', 'actually', 'amazed,', 'this', 'exceeded', 'my', 'expectations.', 'well', 'done!'])\n",
      " ... list(['I', 'love', 'this', 'very', 'much']) list(['mantab'])\n",
      " list(['User', 'Aborted', 'the', 'Operation'])]\n"
     ]
    }
   ],
   "source": [
    "print(doc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(doc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Anyone\n",
      "1 a\n",
      "2 all\n",
      "3 comments?\n",
      "4 making\n",
      "5 of\n",
      "6 rewind\n",
      "7 the\n",
      "8 thought\n",
      "9 This\n",
      "10 is\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in doc_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.40241070225445846),\n",
      " (1, 0.18006326890466595),\n",
      " (2, 0.25183968164741694),\n",
      " (3, 0.5149206417103024),\n",
      " (4, 0.37033443253776116),\n",
      " (5, 0.3769829100798035),\n",
      " (6, 0.15350036231693853),\n",
      " (7, 0.11097708076516127),\n",
      " (8, 0.40241070225445846)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.048*\"A\" + 0.030*\"the\" + 0.015*\"to\" + 0.012*\"a\" + 0.012*\"and\" + 0.011*\"in\" + 0.010*\"I\" + 0.009*\"that\" + 0.008*\"is\" + 0.007*\"rewind\"\n",
      "Topic: 1 \n",
      "Words: 0.029*\"the\" + 0.023*\"this\" + 0.019*\"is\" + 0.016*\"to\" + 0.013*\"of\" + 0.013*\"rewind\" + 0.012*\"This\" + 0.011*\"and\" + 0.010*\"likes\" + 0.010*\"than\"\n",
      "Topic: 2 \n",
      "Words: 0.043*\"the\" + 0.036*\"Than\" + 0.035*\"OriginalBetter\" + 0.030*\"is\" + 0.017*\"this\" + 0.011*\"video\" + 0.011*\"I\" + 0.010*\"This\" + 0.009*\"to\" + 0.009*\"in\"\n",
      "Topic: 3 \n",
      "Words: 0.038*\"BITCH\" + 0.025*\"the\" + 0.020*\"LASAG\" + 0.017*\"rewind\" + 0.015*\"LASAGNA\" + 0.012*\"I\" + 0.010*\"to\" + 0.010*\"this\" + 0.009*\"The\" + 0.008*\"in\"\n",
      "Topic: 4 \n",
      "Words: 0.038*\"I\" + 0.026*\"the\" + 0.020*\"you\" + 0.017*\"and\" + 0.015*\"of\" + 0.014*\"this\" + 0.013*\"is\" + 0.012*\"a\" + 0.012*\"in\" + 0.009*\"it\"\n",
      "Topic: 5 \n",
      "Words: 0.020*\"I\" + 0.016*\"you\" + 0.014*\"the\" + 0.013*\"a\" + 0.009*\"is\" + 0.008*\"and\" + 0.007*\"T\" + 0.007*\"E\" + 0.007*\"to\" + 0.006*\"this\"\n",
      "Topic: 6 \n",
      "Words: 0.023*\"this\" + 0.013*\"is\" + 0.009*\"video\" + 0.009*\"forgot\" + 0.009*\"You\" + 0.008*\"🌑🌑🌑🌑🌑🌑🌑🌑🌑🌑🌑🌑🌑\" + 0.007*\"10\" + 0.006*\"I\" + 0.006*\"rewind\" + 0.006*\"the\"\n",
      "Topic: 7 \n",
      "Words: 0.029*\"rewind\" + 0.021*\"the\" + 0.019*\"is\" + 0.014*\"it\" + 0.013*\"a\" + 0.010*\"this\" + 0.008*\"youtube\" + 0.008*\"in\" + 0.007*\"better\" + 0.006*\"of\"\n",
      "Topic: 8 \n",
      "Words: 0.021*\"this\" + 0.018*\"I\" + 0.015*\"YouTube\" + 0.014*\"the\" + 0.012*\"to\" + 0.011*\"video\" + 0.009*\"on\" + 0.008*\"is\" + 0.008*\"liked\" + 0.007*\"rewind\"\n",
      "Topic: 9 \n",
      "Words: 0.040*\"the\" + 0.030*\"is\" + 0.027*\"rewind\" + 0.020*\"This\" + 0.019*\"this\" + 0.015*\"that\" + 0.014*\"better\" + 0.013*\"was\" + 0.011*\"2018\" + 0.010*\"than\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.007*\"real\" + 0.007*\"the\" + 0.006*\"rewind\" + 0.006*\"I\" + 0.006*\"is\" + 0.005*\"this\" + 0.005*\"This\" + 0.004*\"was\" + 0.003*\"The\" + 0.003*\"and\"\n",
      "Topic: 1 Word: 0.006*\"I\" + 0.004*\"is\" + 0.004*\"the\" + 0.004*\"and\" + 0.004*\"you\" + 0.004*\"to\" + 0.004*\"not\" + 0.003*\"of\" + 0.003*\"|\" + 0.003*\"LASAGNA\"\n",
      "Topic: 2 Word: 0.006*\"is\" + 0.006*\"the\" + 0.005*\"way\" + 0.005*\"better\" + 0.004*\"rewind\" + 0.004*\"This\" + 0.004*\"this\" + 0.004*\"LASAG\" + 0.004*\"I\" + 0.003*\"BITCH\"\n",
      "Topic: 3 Word: 0.016*\"BITCH\" + 0.009*\"LASAG\" + 0.008*\"LASAGNA\" + 0.005*\"the\" + 0.005*\"is\" + 0.005*\"this\" + 0.004*\"you\" + 0.004*\"I\" + 0.003*\"👏👏\" + 0.003*\"LASA\"\n",
      "Topic: 4 Word: 0.009*\"🌑🌑🌑🌑🌑🌑🌑🌑🌑🌑🌑🌑🌑\" + 0.008*\"the\" + 0.006*\"is\" + 0.006*\"Better\" + 0.005*\"rewind\" + 0.005*\"this\" + 0.005*\"I\" + 0.005*\"YouTube\" + 0.004*\"This\" + 0.004*\"2018\"\n",
      "Topic: 5 Word: 0.011*\"best\" + 0.007*\"the\" + 0.007*\"rewind\" + 0.006*\"video\" + 0.006*\"this\" + 0.005*\"in\" + 0.005*\"is\" + 0.004*\"ever\" + 0.004*\"I\" + 0.004*\"The\"\n",
      "Topic: 6 Word: 0.008*\"is\" + 0.008*\"the\" + 0.008*\"rewind\" + 0.007*\"this\" + 0.005*\"like\" + 0.005*\"This\" + 0.005*\"YouTube\" + 0.005*\"likes\" + 0.004*\"I\" + 0.004*\"video\"\n",
      "Topic: 7 Word: 0.018*\"BITCH\" + 0.014*\"LASAGNA\" + 0.008*\"LASAG\" + 0.006*\"this\" + 0.006*\"video\" + 0.005*\"the\" + 0.005*\"I\" + 0.004*\"rewind\" + 0.004*\"is\" + 0.004*\"a\"\n",
      "Topic: 8 Word: 0.008*\"is\" + 0.006*\"the\" + 0.006*\"rewind\" + 0.005*\"this\" + 0.005*\"a\" + 0.004*\"good\" + 0.004*\"4:00\" + 0.004*\"This\" + 0.004*\"xxxtentacion\" + 0.004*\"of\"\n",
      "Topic: 9 Word: 0.008*\"This\" + 0.007*\"than\" + 0.007*\"the\" + 0.007*\"original\" + 0.007*\"better\" + 0.006*\"is\" + 0.005*\"rewind\" + 0.004*\"in\" + 0.004*\"likes\" + 0.003*\"?\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
